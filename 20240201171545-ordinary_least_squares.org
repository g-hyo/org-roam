:PROPERTIES:
:ID:       023bc72c-6ceb-4f94-9f8b-52079592925f
:END:
#+title: ordinary least squares

Ordinary least squares regression seeks to minimise the sum of squared differences between observed responses and predicted responses.


* Linear Model

In a [[id:8a4077cd-e2a9-45c3-acfb-9db0c516e24c][linear model,]] we pick coefficients $\beta$ to minimise the residual sum of squares

\[
RSS(\beta) = \sum_{i=1}^N (y_{i} - x^T_{i}\beta)^2
\]

$RSS(\beta)$ is a quadratic function of the parameters, and hence the minimum always exists. The solution can be characterised in matrix notation.

\[
RSS(\beta) = (y - X\beta)^T(y - X\beta)
\]

where X is an $N \times p$ matrix with each row an input vector, and y is an N-vector of the outputs in the training set. Both terms are essentially the residuals of the model, however the first term is transposed, so that we are essentially getting an [[id:fba474c7-7ea8-4d65-aec1-dd55c6d27361][inner product]] of the vector of residuals with itself.

Recalling that $RSS(\beta)$ is a quadratic function of its parameters, we can use [[id:45d534c1-45f5-4d02-8a29-3bd202688f3d][matrix calculus]] to find the derivative and find its root with:

\[
X^T(y - X\beta) = 0
\]

Assuming $X^TX$ is "nonsingular", i.e., has an [[id:dd8d26c8-f383-4f22-9904-f6130dcb4be0][inverse (matrix)]], then the solution is given by:

\[
\hat{\beta} = (X^TX)^{-1}X^Ty
\]

The fitted value at the ith input is given by:

\[
\hat{y}(x_{i}) = x^T_{i}\hat{\beta}
\]




** Scenarios

Note that if the problem space can be described in terms of two discrete generative processes (i.e., two gaussians generating $y$), then this method is optimal for generating $\beta$.

If however, the problem space is generated by multiple tightly clustered Gaussians, the linear model is unlikely to be optimal.
